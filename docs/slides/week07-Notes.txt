------
Deep Dream
------
Explanation (from???) + example from DLwPython + DD repo example??


------
Style Transfer
------
conserve the "content" of the original image, while adopting the "style" of the reference image. If we were able to mathematically define content and style, then an appropriate loss function to minimize would be the following:

loss = distance(style(reference_image) - style(generated_image)) +
       distance(content(original_image) - content(generated_image))


------
VAE
------
The key idea of image generation is to develop a low-dimensional latent space of representations (which naturally is a vector space, i.e. a geometric space), where any point can be mapped to a realistic-looking image. 

The module capable of realizing this mapping, taking as input a latent point and outputting an image, i.e. a grid of pixels, is called a generator (in the case of GANs) or a decoder (in the case of VAEs). Once such a latent space has been developed, one may sample points from it, either deliberately or at random, and by mapping them to image space, generate images never seen before.

A VAE, instead of compressing its input image into a fixed "code" in the latent space, turns the image into the parameters of a statistical distribution: a mean and a variance. 

Essentially, this means that we are assuming that the input image has been generated by a statistical process, and that the randomness of this process should be taken into accounting during encoding and decoding. 

The VAE then uses the mean and variance parameters to randomly sample one element of the distribution, and decodes that element back to the original input. The stochasticity of this process improves robustness and forces the latent space to encode meaningful representations everywhere, i.e. every point sampled in the latent will be decoded to a valid output.

GANs and VAEs are simply two different strategies for learning such latent spaces of image representations, with each its own characteristics. VAEs are great for learning latent spaces that are well-structured, where specific directions encode a meaningful axis of variation in the data. GANs generate images that can potentially be highly realistic, but the latent space they come from may not have as much structure and continuity.


------
GANs
------
An intuitive way to understand GANs is to imagine a forger trying to create a fake Picasso painting. At first, the forger is pretty bad at the task. He mixes some of his fakes with authentic Picassos, and shows them all to an art dealer. The art dealer makes an authenticity assessment for each painting, and gives the forger feedback about what makes a Picasso look like a Picasso. The forger goes back to his atelier to prepare some new fakes. As times goes on, the forger becomes increasingly competent at imitating the style of Picasso, and the art dealer becomes increasingly expert at spotting fakes. In the end, we have on our hands some excellent fake Picassos.

------
STATUS QUO
------
We haven't discussed some topics such as Reinforcement Learning so if you're interested in those you will have to make your own research

------
PROJECTS
------
Do you want to learn the nuts and bolts of DL? Do you want to understand the general themes and try different implementation?

Interesting themes and area to focus on:
  - Models: Design and tune for performance. Might open a lot of door for your professional life.
  - Interface: very little research and resources on the subject!
  - Concrete usage: A lot of playful usage but little relevant example to solve everyday problems

How is your timeline? You will need more than 2.5 h / week if you want to explore topics in details