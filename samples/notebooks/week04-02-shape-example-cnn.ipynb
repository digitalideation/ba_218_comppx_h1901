{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "To run this code you will need to install [Matplotlib](https://matplotlib.org/users/installing.html) and [Numpy](https://www.scipy.org/install.html)\n",
    "\n",
    "If you like to run the example locally follow the instructions provided on [Keras website](https://keras.io/#installation)\n",
    "\n",
    "It's __strongly__ suggested to use a Python environments manager such as [Conda](https://conda.io/docs/) or some kind of [VirutalEnv](#)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing CNN on our shape classifier\n",
    "\n",
    "Let's try to use a CNN to improve our shape classification model.\n",
    "\n",
    "We create the dataset _manually_ as we did before. By drawing the shapes with the `draw_shape` function that generat some random shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 0 = rectangle, 1 = triangle, 2 = ellipse\n",
    "# return shape\n",
    "def draw_shape(max_size, type):\n",
    "    \n",
    "    # Random size and fixed coordinate\n",
    "#     s = math.floor(random.randrange(1, max_size-4))\n",
    "#     x = math.floor(max_size/2)\n",
    "#     y = math.floor(max_size/2)\n",
    "\n",
    "    # Not so random size and random coordinate\n",
    "    s = int(random.randrange(max_size/2, max_size))\n",
    "    x = int(random.randrange(int(s/2), max_size-int(s/2)))\n",
    "    y = int(random.randrange(int(s/2), max_size-int(s/2)))\n",
    "\n",
    "    type = type%3\n",
    "    \n",
    "    if type == 0:\n",
    "        art = plt.Rectangle((x-s/2, y-s/2), s, s, color='r')\n",
    "\n",
    "    if type == 1:\n",
    "        verts = [\n",
    "            (x-s/2, y-s/2),\n",
    "            (x, y+s/2),\n",
    "            (x+s/2, y-s/2)\n",
    "        ]\n",
    "        art = plt.Polygon(verts, color='r')\n",
    "\n",
    "    if type == 2:\n",
    "        art = plt.Circle((x, y), s/2, color='r')\n",
    "    \n",
    "    return art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a helper function that convert a matplotlib figure to a np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/7821917\n",
    "def fig2rgb_array(fig):\n",
    "    fig.canvas.draw()\n",
    "    buf = fig.canvas.tostring_rgb()\n",
    "    ncols, nrows = fig.canvas.get_width_height()\n",
    "    return np.frombuffer(buf, dtype=np.uint8).reshape(nrows, ncols, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function see if it works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 48, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADsAAAA4CAYAAABDsYAdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAHYQAAB2EBlcO4tgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAELUlEQVRoge2aT2gUVxzHPz9pIOlJ9NaGJjmWgvXgqcci9FToLcSjUKHFRBPRjYJRJKG71FpRiBEhSoumUM1FVEoWQXIJYiBWECsSjJldFf/EP6vJ6mZ/Hl6i22QnOzP7ZmeJ+cCDnXm/9/u9785j3nu/N6KqfCysiroDlWRF7HLlE1uOWlpatKGhwZY7KyQSiQFV3fT+hqpaKbFYTKsNIK4Ffay+YTw1BXv2hOK6+sR2dsLRo3D1qnXX1SX20SO4eBEyGdi2zbr76hK7cyc4jvl9+zYMD1t1Xz1i79+HZPLD9dOn0N5uNUT1iG1vh1Tq//fGx2FoyFqI6hB7717xITs1ZYa2pfV7dYjdvh3S6eJ1d+/ChQtWwkQvdnwcRkbc658/h927rTzd6MW2tpqX01JMTMDZs2WHilbsrVswOlra7uVL6OqCfL6scNGKbW2Fhw+92ToOnD5dVrjoxN64Adeve7fPZKC7G2ZnA4eMTuzWrWZ56AfHgf7+wCGLihWR70XkhIj8LSJfich/ItInIj8EjlTItWtw86b/dq9fQyIBuVygsEXFqup5Vf0R6Aa+AzLAp4ATKMpC2trg8eNgbVMpOHYsWFt12Yxj/og+4DNAgFpgoIjdRiDe3NzsbUc9PKy6dq2qmTmDlaYm1WzWzuZdRAT4DTihqum5tjPAopldVZOq2tnY2Ojt3+3ogCdPfD6SBaTTcPiw72ZuOag24BugTkS+Bb4EaoDBwB0EuHwZ7twpywUA2Sz09Zmpq67Oezu3Yey3lMxB5fOq69eXN3wLS02N6v795Q/jULh0ySzqbfH2LZw8Ca9eeW5SGbGqEIvBs2d2/aZSZqHhkcqIHRw0e1bb5HJw5gy8eOHJPHyx+Tzs3eu5Q75xHNi3z5Np+GIHBmByMjz/+TycO2eyGiUIV+zsLBw4YBbxYeI4ZoNfgnDFnjr1ITUaJqomdVNiYxGe2FwO4nGzeK8EjgO7di1pEp7Y48cXp0bDZmhoyRRPOGLfvIFDh2B6OhT3rqRSsGOHa3U4Yo8cqfxTnefKFde3v32xMzPQ22sW61GQTrseitkXe/BgdE91npERk49egKfPDESkHvhl7vJXVf23qKGqeapr1gTtph2yWejpWXTb6zcVm4Eu4AFmU/9zUSsRGBuLbggXsnr1ouScV7GfA5OqmhOR2sIKEdmISc2MJRKJf6x0FBqACQt+vii88Co2BdSLyANgprBCVZNAUkRQ1U4LHURE4rZ8FeJVbD/Qg8lB/e5ik3S5HwSbvt4juvLt4vKkbLEiUi8if86VdWX4CfcUAsrPLmKmpCagDui14O9roAMYBf4ANtjKgNoYxvPT0jTm1CAwIrIK+An4C9gAbAHcV/Y+sSF2flqqZcG05Ac/pxBBsfFVqpdpyQvhnEIUsDL1LFdWxC5XPiqx7wA7TgUZBhrVGQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 48x48 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Image and dataset size we are going to use\n",
    "image_size = 48\n",
    "dataset_size = 5000\n",
    "\n",
    "# Create plot's figure and axes\n",
    "# https://stackoverflow.com/a/638443\n",
    "fig = plt.figure(figsize=(1,1), dpi=image_size)\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Setting for the axes\n",
    "ax.set_xlim(0,image_size)\n",
    "ax.set_ylim(0,image_size)\n",
    "# ax.axis('off')\n",
    "\n",
    "# Draw a random shape\n",
    "art = draw_shape(image_size,random.randint(0,2))\n",
    "# Add the shape to the plot\n",
    "# https://stackoverflow.com/a/29184075\n",
    "plt.gcf().gca().add_artist(art)\n",
    "# gcf() means Get Current Figure\n",
    "# gca() means Get Current Axis\n",
    "\n",
    "# convert the figure to an array\n",
    "data = fig2rgb_array(fig)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a loop that will generate a small dataset for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 48, 48, 3)\n",
      "(5000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADsAAAA4CAYAAABDsYAdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAHYQAAB2EBlcO4tgAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAAelJREFUaIHtms1KW0EYhp/XRI3RkEJFxHPQrZfRTaVuxFVx489C3Kgbs5AgIqJS2rrp3fQSeiHehqSL6YFUjCfnnPmBcZ5dyHzzzXNeZk4GotFoxHthJvQCfJJkYyXJxkqSjZUkGytJNlaSbKwkWe9IX5D+IMllm/CyRvAHsAl8dtkqvCzsAutAH/jpMt2wskbsASMKsAHsuGoXOtk9IB/73AceXKUbTlZqATdA78U368BXFy1DJnvI/6kW9IBbJOtrCyMrtYEhsDhhRAbs224bKtkTjNAkloCrfw/FGv5lpTngAlgoGZkBxzZbh0j2jLdTLegCA6RZW439ykodjGxnyoocOLXV3neyA6ZLtaADnCPN22juT1bqYvbgXMXKDLPHG+Mz2SGvv1fLmAdOkMoOtFL8yEo94ACo+yrJgMumy/CV7A3V9upLZoEjpKUmi3AvK33A/NZtNZwpA66bTOAj2XuapVrQBvaQ+qUjJ+BWVlrG3E9t9cmBu7rFrpP9Tr0TeBIzwC7Sx7rFbpBWgS3A9kU8B77VKXSZ7CN2Uy0QsI20UrXQjayUA5+czG3IMQ+zEq6S/YWdE/gttpHWqhRYvRyP8Rt4cjT3OM9VBiv96StSkmysJNlYSbKxkmRjJcnGSpKNlXcl+xcr/yLU0nak2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 48x48 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_dataset(image_size, dataset_size):\n",
    "\n",
    "    # Those variable will contain the images and associated labels\n",
    "    images = np.zeros((dataset_size, image_size, image_size, 3))\n",
    "    labels = np.zeros((dataset_size))\n",
    "    \n",
    "    # The plot figure we will use to generate the shapes\n",
    "    fig = plt.figure(figsize=(1,1), dpi=image_size)\n",
    "\n",
    "    for i in range(dataset_size):\n",
    "        \n",
    "        # Clear the figure\n",
    "        fig.clf()\n",
    "        \n",
    "        # Recreate the axes\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.set_xlim(0, image_size)\n",
    "        ax.set_ylim(0, image_size)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Define label\n",
    "        label = i%3\n",
    "        art = draw_shape(image_size, label)\n",
    "        plt.gcf().gca().add_artist(art)\n",
    "        \n",
    "        # Add values to the arrays\n",
    "        images[i] = fig2rgb_array(fig)\n",
    "        labels[i] = label\n",
    "        \n",
    "    return images, labels\n",
    "\n",
    "# Generate our dataset\n",
    "images, labels = generate_dataset(image_size, dataset_size)\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually we can save our dataset for later, since it takes quite some time to generate it ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('datasets/shape-example-shapes4.npy', images)\n",
    "# np.save('datasets/shape-example-labels1.npy', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to load it we can then use the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.load('datasets/shape-example-shapes1.npy')\n",
    "labels = np.load('datasets/shape-example-labels1.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our dataset manually in training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 48, 48, 3)\n",
      "(4000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14a0b5750>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the size of the training set, here we use 80% of the total samples for training\n",
    "train_size = int(dataset_size*.8)\n",
    "\n",
    "# TODO: We should shuffle the dataset\n",
    "\n",
    "# Split the dataset into train and test dataset\n",
    "train_images, test_images = images[:train_size], images[train_size:]\n",
    "train_labels, test_labels = labels[:train_size], labels[train_size:]\n",
    "\n",
    "# Verify the data\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "# sample_images = []\n",
    "# for label, image in list(zip(train_labels, train_images))[:10]:\n",
    "#     fig1, ax1 = plt.subplots()\n",
    "#     ax1.axis('off')\n",
    "#     plt.title(label)\n",
    "#     fig1.add_subplot(111).imshow(image/255)\n",
    "\n",
    "full_image = np.concatenate(train_images[:12]/255, axis=1)\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.imshow(full_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1), \n",
    "                       activation='relu', input_shape=(image_size, image_size, 3)))\n",
    "network.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "network.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "network.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "network.add(layers.Flatten())\n",
    "network.add(layers.Dense(1000, activation='relu'))\n",
    "network.add(layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in the `[0, 1]` interval. Then we also need to categorically encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 48, 48, 3)\n",
      "(1000, 48, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "train_images = train_images.astype('float32') / 255\n",
    "print(train_images.shape)\n",
    "test_images = test_images.astype('float32') / 255\n",
    "print(test_images.shape)\n",
    "\n",
    "# Encode to categorical\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 6.1012e-04 - accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "4000/4000 [==============================] - 10s 3ms/step - loss: 3.5986e-04 - accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 2.2147e-04 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4000/4000 [==============================] - 10s 3ms/step - loss: 1.6545e-04 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4000/4000 [==============================] - 10s 2ms/step - loss: 1.2895e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14a407810>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 656us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc: 0.9990000128746033\n"
     ]
    }
   ],
   "source": [
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "# get_image will return a handle to the image itself, and a numpy array of its pixels to input the network\n",
    "def get_image(path):\n",
    "    img = image.load_img(path, target_size=network.input_shape[1:3])\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    return img, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import PIL.Image. The use of `load_img` requires PIL.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8d16e447a62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"assets/photo_rect_red01.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-fe1c75559c04>\u001b[0m in \u001b[0;36mget_image\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# get_image will return a handle to the image itself, and a numpy array of its pixels to input the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/notebooks/lib/python3.7/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mcolor_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpil_image\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         raise ImportError('Could not import PIL.Image. '\n\u001b[0m\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m    110\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import PIL.Image. The use of `load_img` requires PIL."
     ]
    }
   ],
   "source": [
    "img, x = get_image(\"assets/photo_rect_red01.jpg\")\n",
    "predictions = network.predict(x)\n",
    "plt.imshow(img)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding data augmentation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
